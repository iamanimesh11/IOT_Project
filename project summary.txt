For this project we have two system, business logic,and backend server logic.
so what we are trying to do is simulate a real time etl of iot data .

So both system are simulated , now what happen and let's understand how data flows basically ,
So iot device like refrigerator,ac ,tv and so on generates data  like sensors data more basically as we know how iot works . 
so suppose a event like motor failure occurs and so it send to backend server .now backend server receive it .
by the time understand how business logic is working First:
a service  regsiter to backend server and it suscribe to backend sever for each device id to receive the real time events of that event 
.examplew:
service A register at backend server for device id ref_model_12 of device type refrigerator and device model "model1"
now backend server make sure every event being recieved by device alloted deivce id will send event data to that service.
now for now that is basically a event simulation .

Now how tech stack works here ? how i have configured it ?
 so backend server contains following componenets
 1.Service registration fast api:so whenever a route is caled by post method ,backend receive it and store service detail like
   name,key,callback url,timestamp.also api return important data like service key,id which is generated by pythons function.
2. device data generator: generate mock device data based on n where n is number of devices and put them on fast api .
device data is like: device_id,device_type,model_name,reportable BOOLEAN DEFAULT TRUE,created_at TIMESTAMP DEFAULT now()
3.customer data generator:so once mock device data is generated ,a mock customer data is generated as well referncing each device id
Structure look like this:token,device_id,issued_at,expires_at,is_active
4. Now it comes to subscription logic in backend:
whenver client/service  make a call on POST method by backend script handle the logic of token verification in it which was used in call
parameters,so that authenticaton can be made ,now if verified store service id and key in redis corresponding with device_id
for faster lookup up in future  and so it sends following things to kafka topic named as subscription topic:
 "service_id": service_id,"device_id": device_id,"timestamp": now_utc.isoformat(),"expiry_hours": expiry_hours
now consumer will consume it and will sotre in subscription table as well in reds,
subscription table strucutre look like:
id SERIAL PRIMARY KEY,service_id UUID NOT NULL REFERENCES subscriptions.services(service_id), 
device_id VARCHAR(255) NOT NULL,subscribed_at TIMESTAMP DEFAULT NOW(),expires_at TIMESTAMP NOT NULL,subscription_status
concluding this so now we will know which service has susbcribed to what device_id. 

5. Event-simulation:
most main and major component part of project now is event simulation.
my approach for this was :
1. as we know we have devices data in table , and also device_profile json file which will helpsus to generate telemetry/mock data
find json file structre below:
{
  "Refrigerator": {
    "status": ["idle", "cooling", "defrosting"],
    "errors": ["CF", "CH", "CL", "C0", "CO2", "C5", "dH", "d5"]
  },
  "Television": {
    "status": ["off", "on", "standby"],
    "errors": ["T01", "T03", "T09", "SLEEP_ERR"]
  }
  so on  /....................
}
so a data is generated and sent to mqtt topic, topic=f"iot/telemetry/{device['device_type']}/{device['device_id']}"
strucutre is like this:payload = {
        "device_id": device["device_id"],
        "device_type":device["device_type"],
        "model_name": device["model_name"],
        "status": random.choice(status_options),
        "error_code": error,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}

so a generator script basically generate telemetry data and send to topic.
Now I have mqtt_consumer_Controller script which basically create multiple mqtt consumers based on device type and so each mqtt consume
will consume mesage from each device type like refrigerator,tv,ac and so on.
its just for scalable architecurtre to handle messages continously.

Now MQTT consumer will forward the message to kafka topic named as same of devcie type for unqiely identifciation.
similarly like mqtt ,kafka controller script will create multiple consumer containers for each kafka topic .
and each consumer has logic for device type processing logic like forward to service callback url for that device id,store in database 
whatever it is .

till now we have logic to generate telemetnry event data and sned it via mqtt to kafka consumer and it will handle logic further .
Enough for backend logic here.

** now lets comes to front busines logic .**
a service has registered itself in backend server with paramter like 
#curl -X POST http://192.168.1.5:5001/register -H "Content-Type: application/json" -d "{\"service_name\": \"MyNewService\", \"callback_url\": \"http://my-service-host/api/callback\"}"

now service is ready and it will wait for event to recieve .
endpoint webhook (basically a c allback_url) is running and will receive events here.
so suppose backend send event to this service now what happens??
it will receive and use it for own use .
here use case is send event to crm(mock crm system running on url  uvicorn.run(app, host="0.0.0.0", port=8002)) .

now what mock_Crm do?
its just a mock stage like one who manage crm system must be able to see which device id is getting error and so inform
the corresponding  user as we have alreay custoemr data too.

So summing up whole live a its devices event simulation with subscribed services(respective device id)
