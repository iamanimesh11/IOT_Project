# âš¡ Real-Time IOT ETL for Reactive and Predective Maintenance

> ğŸ›°ï¸ An end-to-end real-time data engineering pipeline to simulate real-world IoT device events with scalable backend processing simulation, service subscriptions, and real-time event delivery to CRM  using modern data engineering tools.

---
**Remarks**:  
In real-world Data Engineering projects, deploying a full-scale production setup can be costly. Therefore, for the purpose of showcasing, the entire infrastructure in this project is built and demonstrated locally using Docker â€” ensuring it's fully reproducible without incurring any extra cost.

---



## ğŸ“š Table of Contents

- [Key Features](#-key-features)
- [Tech Stack](#%EF%B8%8Ftech-stack)
- [Getting Started](#-getting-started)
- [Prerequisites](#-prerequisites)
- [Architecture](#architecture)
- [Setup Instructions](#setup-instructions)
- [Directory Structure](#directory-structure)
- [Configurations](#configurations)
- [Logging & Monitoring](#-logging--monitoring)
- [Screenshots](#screenshots)
- [Future Scope](#-future-scope)
- [Author](#-author)


## ğŸ”‘ Key Features

- **ğŸ³ Fully Dockerized Architecture**  
  Deploy the entire stack with a single `docker-compose up --build` â€” no manual setup.

- **âš™ï¸ Real-Time IoT Event Simulation**  
  Continuously generates mock telemetry data from simulated IoT devices like Refrigerators, TVs, and Washing Machines..

- **â° Airflow-Based Workflow Orchestration**  
Airflow is integrated to schedule background tasks such as data validation, expired subscription cleanup, and periodic telemetry batch processing.
  
- **ğŸ“¡ Apache Kafka for High-Throughput Streaming**  
  Events are routed through Kafka, with device-type-specific consumer containers for modular and scalable processing.

- **ğŸ› ï¸ FastAPI Backend for Service Registration , Subscriptiong,webhook**  
    Enables external services to register and subscribe to specific device IDs for real-time event delivery.

- **ğŸ“ JSON, PostgreSQL, and Redis Integration**
   Combines persistent storage (PostgreSQL), structured configurations (JSON), and in-memory speed (Redis).

- **ğŸ“ Centralized Logging with Loki**  
  All logs from Python apps and services tasks are sent to Grafana Loki for monitoring and troubleshooting.

- **ğŸ“Š Visual Monitoring with Grafana**  
Real-time visualization of system metrics, device errors, and subscription statuses using Grafana dashboards.

- **ğŸ”” Notification System (Optional)**  
  Sends ETL job alerts (success/failure) via Discord webhooks.

- **ğŸ” Secure Credential & API Key Management**  
  Firebase securely stores API keys, secrets, and credentials â€” no hardcoding.

- **ğŸ’¾ Persistent PostgreSQL Storage**  
  Maintains structured data and ensures durability across restarts.

- **ğŸ§ª Mock CRM Integration**  
    Simulates CRM system behavior by receiving device alerts and acting on customer-device mappings.
  
- **ğŸ‘¨â€ğŸ’» Plug-and-Play for Recruiters**  
  Instantly clonable and runnable â€” ideal for technical demos or code evaluations.

---
# ğŸ› ï¸Tech Stack

| Component      | Tool / Service        | Logo                              |
|----------------|-----------------------|-----------------------------------|
| **Data Source** | Mock data generated via python | <img src="https://s3.dualstack.us-east-2.amazonaws.com/pythondotorg-assets/media/community/logos/python-logo-only.png" alt="Python" width="70"/>|
| **Scheduler**  | Apache Airflow         | <img src="https://icon.icepanel.io/Technology/svg/Apache-Airflow.svg" alt="Airflow" width="70"/> |
| **Streaming**  | Apache Kafka           | <img src="https://irisidea.com/wp-content/uploads/2024/04/kafka-implementation-experience--450x231.png" alt="Kafka" width="120"/> |
| **Storage**    | PostgreSQL             | <img src="https://www.logo.wine/a/logo/PostgreSQL/PostgreSQL-Logo.wine.svg" alt="PostgreSQL" width="120"/> |
| **Logging**    | Grafana Loki           | <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Grafana_logo.svg/2005px-Grafana_logo.svg.png" alt="Grafana Loki" width="100"/> |
| **Mmeory Based Storage**    | Redis  | <img src="[https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Grafana_logo.svg/2005px-Grafana_logo.svg.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRdtAvGoJcM9g2e771ie7AmJfeZ_SQG-BrGYw&s)" alt="Redis" width="100"/>|
| **UI framework**    | Streamlit           | <img src="https://streamlit.io/images/brand/streamlit-logo-primary-colormark-darktext.png" alt="Streamlit" width="180"/> |
| **Containerization**  | Docker, Docker Compose | <img src="https://cdn4.iconfinder.com/data/icons/logos-and-brands/512/97_Docker_logo_logos-1024.png" alt="Docker" width="100"/>|
| **Alerts and other**   | Discord               | <img src="https://pngimg.com/uploads/discord/discord_PNG3.png" alt="Discord" width="110"/>|
| **Language**   | Python                 | <img src="https://s3.dualstack.us-east-2.amazonaws.com/pythondotorg-assets/media/community/logos/python-logo-only.png" alt="Python" width="70"/>|

---

# Architecture

![Workflow](asset/workflow.gif)


# ğŸš€ Getting Started


## âœ… Prerequisites

Before running this project locally, make sure you have the following installed on your system:

- [Docker](https://www.docker.com/products/docker-desktop) & [Docker Compose](https://docs.docker.com/compose/)
- [Git](https://git-scm.com/downloads)
- A code editor like [VS Code](https://code.visualstudio.com/)
- Internet connection to access external APIs (TomTom, WeatherAPI, etc.)

ğŸ’¡ **Note:**  
Ensure that your systemâ€™s firewall or antivirus isnâ€™t blocking Docker containers from making network requests.


# Setup Instructions


###  Clone the Repository

First, clone the repository to your local machine:

```bash
git clone https://github.com/animesh11singh/project_real_time_trafic_monitoring.git
cd project_real_time_trafic_monitoring
```
### Run in terminal

```bash
docker-compose up -d --build
```
## ğŸ“‚Directory Structure

```bash
.
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
â”œâ”€â”€ additionals
â”‚   â”œâ”€â”€ backup.sqlc
â”‚   â”œâ”€â”€ project_structure.json
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md
â”‚   â”œâ”€â”€ project_structure_json_creator.py
â”‚   â””â”€â”€ text_Search.py
â”œâ”€â”€ common
â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ common
â”‚   â”‚   â”œâ”€â”€ logging_and_monitoring
â”‚   â”‚   â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ loki_errors.text
â”‚   â”œâ”€â”€ credentials
â”‚   â”‚   â”œâ”€â”€ config.ini
â”‚   â”‚   â”œâ”€â”€ firebase_cred.json
â”‚   â”‚   â””â”€â”€ red-button-442617-a9-89794f0a5b90.json
â”‚   â”œâ”€â”€ logging_and_monitoring
â”‚   â”‚   â”œâ”€â”€ centralized_logging.py
â”‚   â”‚   â””â”€â”€ firebase_db_api_utils.log
â”‚   â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â”‚   â”œâ”€â”€ api_utils.log
â”‚   â”‚   â”‚   â”œâ”€â”€ database_connection.log
â”‚   â”‚   â”‚   â”œâ”€â”€ db_utils.log
â”‚   â”‚   â”‚   â”œâ”€â”€ firebase_db_api_utils.log
â”‚   â”‚   â”‚   â”œâ”€â”€ kafka_consumer.log
â”‚   â”‚   â”‚   â”œâ”€â”€ loki_errors.text
â”‚   â”‚   â”‚   â”œâ”€â”€ road_data_main.log
â”‚   â”‚   â”‚   â”œâ”€â”€ Road_Producer.log
â”‚   â”‚   â”‚   â”œâ”€â”€ Traffic_consumer.log
â”‚   â”‚   â”‚   â””â”€â”€ Traffic_Producer.log
â”‚   â”œâ”€â”€ streamlit
â”‚   â”‚   â”œâ”€â”€ database_logger dashboard-7-4.json
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ Docker_container_Status.py
â”‚   â”‚   â”œâ”€â”€ Docker_running_containers_HTTP_Streamlit.py
â”‚   â”‚   â”œâ”€â”€ ETL_walkthrough_Streamlit.py
â”‚   â”‚   â”œâ”€â”€ kafka_manager_Streamlit.py
â”‚   â”‚   â”œâ”€â”€ lokii_streamlit.py
â”‚   â”‚   â”œâ”€â”€ main_Streamlit.py
â”‚   â”‚   â”œâ”€â”€ network_utils.py
â”‚   â”‚   â”œâ”€â”€ PostgreSQL_streamlit_app.py
â”‚   â”‚   â”œâ”€â”€ project_flow.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ images
â”‚   â”‚   â”‚   â”œâ”€â”€ Daasboard_1.png
â”‚   â”‚   â”‚   â”œâ”€â”€ Grafana_guide_1.png
â”‚   â”‚   â”‚   â”œâ”€â”€ Grafana_guide_2.png
â”‚   â”‚   â”‚   â””â”€â”€ Grafana_guide_3.png
â”‚   â”‚   â”œâ”€â”€ loki_files
â”‚   â”‚   â”‚   â”œâ”€â”€ log_sent _to_loki.py
â”‚   â”‚   â”‚   â””â”€â”€ loki_request_python.py
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”œâ”€â”€ api_utils.py
â”‚   â”‚   â”œâ”€â”€ config_loader.py.py
â”‚   â”‚   â”œâ”€â”€ Database_connection_Utils.py
â”‚   â”‚   â”œâ”€â”€ db_utils.py
â”‚   â”‚   â”œâ”€â”€ extract_Data_from_link_using_DIFFBOT.py
â”‚   â”‚   â”œâ”€â”€ firebase_db_api_track_util.py
â”‚   â”‚   â”œâ”€â”€ genai_text_Extracter.py
â”‚   â”‚   â”œâ”€â”€ kafka_modify_Topics_utils.py
â”‚   â”‚   â””â”€â”€ trafficHelper_utils.py
â”œâ”€â”€ config
â”‚   â”œâ”€â”€ init-db.sql
â”‚   â”œâ”€â”€ loki-config.yml
â”‚   â”œâ”€â”€ loki.json
â”‚   â”œâ”€â”€ promtail-config.yml
â”‚   â””â”€â”€ wait-for-flag.sh
â”œâ”€â”€ grafana
â”‚   â”œâ”€â”€ provisioning
â”‚   â”‚   â”œâ”€â”€ dashboards
â”‚   â”‚   â”‚   â”œâ”€â”€ Airflow Log Analytics.json
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard.yml
â”‚   â”‚   â”‚   â””â”€â”€ ETL dashboard.json
â”œâ”€â”€ pipelines
â”‚   â”œâ”€â”€ airflow
â”‚   â”‚   â”œâ”€â”€ airflow.cfg
â”‚   â”‚   â”œâ”€â”€ airflow.db
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ webserver_config.py
â”‚   â”‚   â”œâ”€â”€ dags
â”‚   â”‚   â”‚   â”œâ”€â”€ DAG_kafka_road_producer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ DAG_kafka_traffic_producer.py
â”‚   â”‚   â”‚   â””â”€â”€ DAG_Monitor_ETL_health.py
â”‚   â”‚   â”œâ”€â”€ plugins
â”‚   â”œâ”€â”€ kafka_files
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ Dockerfile_consumer_traffic
â”‚   â”‚   â”œâ”€â”€ modify_Topics.py
â”‚   â”‚   â”œâ”€â”€ requirements_traffic.txt
â”‚   â”‚   â”œâ”€â”€ road_consumer.py
â”‚   â”‚   â”œâ”€â”€ road_producer.py
â”‚   â”‚   â”œâ”€â”€ traffic_consumer.py
â”‚   â”‚   â”œâ”€â”€ traffic_producer.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ scripts
â”œâ”€â”€ shared
â”‚   â””â”€â”€ wait-for-flag.sh

```


### â–¶ï¸ Next Steps

Once the project is up and running, follow these steps:

1. ğŸŒ Open your browser and visit:  
   **[http://localhost:8501/](http://localhost:8501/)**  
   This will open the ETL helper streamlit app .

2. ğŸ“‹ **Follow the ETL instructions** provided on the strreamlit app to Simulate ETL step by step.

3. ğŸ³ **Keep an eye on your containers:**  
   Use `docker ps` or Docker Desktop to monitor the status of all services.

---

âœ… Everything running smoothly? You're all set to explore the project!







## Configurations 

### Access the Services
Once the containers are up and running, you can access the following services :

| Service           | URL                           | Username | Password |
|-------------------|-------------------------------|----------|----------|
| Streamlit App     | [http://localhost:8501](http://localhost:8501) | _N/A_     | _N/A_     |
| Airflow UI        | [http://localhost:8080](http://localhost:8080) | `animesh` | `animesh16` |
| Grafana Dashboard for logs | [http://localhost:3000](http://localhost:3000) | `admin`   | `animesh16` or `admin`   |

Postgrsql Database initialized at startup of Postgresql container with default configurations.

---


## ğŸ“Š Logging & Monitoring


This project implements a **centralized logging and monitoring system** using **Grafana Loki**, ensuring transparency, debuggability, and maintainability across all services.

### Key Highlights

- **Structured Logging**  
  All Python scripts across Kafka producers/consumers, Airflow DAGs, and data pipelines generate structured logs with timestamp, service name, event type, and status.

- **Centralized Collection with Grafana Loki**  
  Logs from all services are collected and pushed to Loki using `Promtail`. These logs are accessible in real-time via **Grafana dashboards**.

- **Dockerized Monitoring Stack**  
  - `Grafana` for visualization  
  - `Loki` for log storage  
  - `Promtail` for log shipping  
  These services are configured in `docker-compose.yml` with persistent volume storage.

- **Real-Time Debugging**  
  Logs include all critical operations such as:
  - API calls (Overpass, TomTom, WeatherAPI)  
  - Kafka message flow  
  - Database operations (insert/update/failure)  
  - Retry attempts and error messages  

- **Failover & Local Storage**  
  In case of Grafana/Loki downtime, logs are safely written to local files and retried later to avoid data loss.

- **Security & Hygiene**  
  - API keys and sensitive values are **excluded from log outputs**  
  - Logs are rotated and archived periodically (based on configuration)

### Accessing Logs

1. Navigate to [http://localhost:3000](http://localhost:3000)
3. Use log query labels like `{job="airflow"}` or `{job="kafka-producer"}` to filter logs
4. Dashboard panels show service-wise activity, recent errors, and API request status

ğŸ“· **Please find sample images of dashboards and logs below**



> âœ… This setup ensures end-to-end visibility into your ETL pipeline operations.

## ğŸ—ƒï¸ Data Stored

| Table Name       | Description                      |
|------------------|----------------------------------|
| roads_traffic     | Road metadata from Overpass API |
| traffic_flow_data | Real-time traffic speed data    |
| weather_conditions| Weather data per coordinate     |

---


## ğŸ“Š Future Scope
Great! Here's a **"ğŸš€ Future Scope"** section you can include in your README file to highlight the possible extensions and advanced features of your project:

---

## ğŸš€ Future Scope

This project serves as a strong foundation for simulating real-time IoT event-based ETL systems. Below are some future enhancements that can further elevate its capabilities:

### ğŸ” 1. **Real-Time Stream Processing**

* Integrate **Apache Flink** or **Spark Structured Streaming** to perform low-latency processing on telemetry data.
* Enable complex event detection like anomaly spotting, pattern recognition, or sliding-window aggregations.

### ğŸ¤– 2. **Predictive Maintenance with Machine Learning**

* Train and deploy ML models to **predict equipment failures** using historical telemetry.
* Schedule model training and inference using Airflow DAGs.
* Send early warnings to subscribed services or CRM systems.

### â˜ï¸ 3. **Cloud Integration & Data Lake**

* Push data to cloud platforms like **AWS S3**, **Google Cloud Storage**, or **Azure Blob** for long-term storage.
* Store telemetry in **Parquet** format with partitioning for better query performance and future analytics.

### ğŸ“Š 4. **Advanced Monitoring & Alerting**

* Extend Grafana dashboards to include:

  * Service health
  * Kafka consumer lag
  * Device-specific error trends
* Integrate **Prometheus alerts** to notify on failures, lags, or inactive devices.

### ğŸ“¦ 5. **CI/CD and DevOps**

* Automate Docker builds and deployments using **GitHub Actions** or **GitLab CI**.
* Ensure production-grade system reliability and faster iterations.

### ğŸ§‘â€ğŸ’» 6. **Admin & CRM Dashboard**

* Build a **React/Next.js** based web interface to:

  * Visualize live device errors
  * Manage service subscriptions
  * View device-to-customer mappings

### ğŸ”„ 7. **Event Replay and Reprocessing**

* Build a Kafka replay module for testing and backfilling ML or ETL jobs.
* Useful for simulating new use cases on existing historical telemetry.

### ğŸ” 8. **Enhanced Security**

* Add authentication and authorization for:

  * Device data ingestion
  * Service registration and callbacks
* Use JWT tokens or OAuth2 for secure communications.

---

Remarks :
As we know in Data Engineering project,its impossible to bear cost of production and only way is to do everything locally for showcasing a project.


## ğŸ‘¤ Author

- **[Animesh Singh]**
- ğŸ’¼ Aspiring Data Engineer | Big Data |Python | Postgresql/Databases| Kafka | Airflow | Docker 
```

----------------------------------------------------------------------------
